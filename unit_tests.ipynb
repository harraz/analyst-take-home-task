{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ee8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession, functions as Func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def spark():\n",
    "    spark = SparkSession.builder.master(\"local[2]\").appName(\"unitTest\").getOrCreate()\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "\n",
    "def test_filtering_criteria(spark):\n",
    "    # Create a sample DataFrame with minimal columns needed\n",
    "    data = [\n",
    "        # patient, enc_START, enc_STOP, overdose_flag, birthdate\n",
    "        (\"patient_1\", \"2000-01-01 10:00:00\", \"2000-01-01 12:00:00\", \"overdose\", \"1980-01-01 00:00:00\"),\n",
    "        (\"patient_2\", \"1999-07-01 10:00:00\", \"1999-07-01 12:00:00\", \"overdose\", \"1985-01-01 00:00:00\"),  # before cutoff\n",
    "        (\"patient_3\", \"2001-01-01 10:00:00\", \"2001-01-01 12:00:00\", \"non-overdose\", \"1990-01-01 00:00:00\"),  # wrong type\n",
    "        (\"patient_4\", \"2000-05-01 10:00:00\", \"2000-05-01 12:00:00\", \"overdose\", \"1960-01-01 00:00:00\")   # older than 35 at encounter \n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"PATIENT\", StringType(), True),\n",
    "        StructField(\"enc_START\", StringType(), True),\n",
    "        StructField(\"enc_STOP\", StringType(), True),\n",
    "        StructField(\"encounter_type\", StringType(), True),\n",
    "        StructField(\"BIRTHDATE\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "    # Assume your filtering logic is in a function called build_cohort(df) that returns the filtered DataFrame\n",
    "    # For demonstration, here is a simplified version:\n",
    "    filtered = df.filter(\n",
    "        (Func.col(\"encounter_type\") == \"overdose\") &\n",
    "        (Func.col(\"enc_START\") > Func.lit(\"1999-07-15 00:00:00\"))\n",
    "    )\n",
    "    \n",
    "    res = filtered.collect()\n",
    "    assert len(res) == 1\n",
    "    assert res[0][\"PATIENT\"] == \"patient_1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bb4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_readmission_indicators(spark):\n",
    "    data = [\n",
    "        # PATIENT, enc_START, enc_STOP\n",
    "        (\"patient_1\", \"2022-01-01 08:00:00\", \"2022-01-01 10:00:00\"),\n",
    "        (\"patient_1\", \"2022-01-15 09:00:00\", \"2022-01-15 11:00:00\"),\n",
    "        (\"patient_1\", \"2022-04-01 09:00:00\", \"2022-04-01 11:00:00\")  # > 90 days difference from previous row\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"PATIENT\", StringType(), True),\n",
    "        StructField(\"enc_START\", StringType(), True),\n",
    "        StructField(\"enc_STOP\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    # Convert date strings to timestamp, if your code does it within the transformation, adjust accordingly.\n",
    "    df = df.withColumn(\"enc_START\", Func.col(\"enc_START\").cast(TimestampType())) \\\n",
    "           .withColumn(\"enc_STOP\", Func.col(\"enc_STOP\").cast(TimestampType()))\n",
    "    \n",
    "    # Create a window partitioned by PATIENT and ordered by enc_START\n",
    "    patient_window = Window.partitionBy(\"PATIENT\").orderBy(Func.col(\"enc_START\"))\n",
    "    \n",
    "    # Compute next_enc_START and diff_days as in your notebook\n",
    "    df = df.withColumn(\"next_enc_START\", Func.lead(\"enc_START\").over(patient_window))\n",
    "    df = df.withColumn(\"diff_days\", Func.datediff(Func.col(\"next_enc_START\"), Func.col(\"enc_START\")))\n",
    "    \n",
    "    # Flag readmissions: 90-day and 30-day indicators\n",
    "    df = df.withColumn(\n",
    "        \"READMISSION_90_DAY_IND\",\n",
    "        Func.when((Func.col(\"diff_days\").isNotNull()) & (Func.col(\"diff_days\") != 0) &\n",
    "                  (Func.col(\"diff_days\") <= 90), Func.lit(1)).otherwise(Func.lit(0))\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"READMISSION_30_DAY_IND\",\n",
    "        Func.when((Func.col(\"diff_days\").isNotNull()) & (Func.col(\"diff_days\") != 0) &\n",
    "                  (Func.col(\"diff_days\") <= 30), Func.lit(1)).otherwise(Func.lit(0))\n",
    "    )\n",
    "    \n",
    "    # Update next_enc_START to \"N/A\" if diff_days is 0 (simulate that transformation)\n",
    "    df = df.withColumn(\n",
    "        \"next_enc_START\",\n",
    "        Func.when((Func.col(\"diff_days\") <= 90) & (Func.col(\"diff_days\") != 0), Func.col(\"next_enc_START\")).otherwise(Func.lit(\"N/A\"))\n",
    "    )\n",
    "    df = df.withColumnRenamed(\"next_enc_START\", \"FIRST_READMISSION_DATE\")\n",
    "    \n",
    "    results = df.collect()\n",
    "    \n",
    "    # Assertions:\n",
    "    # Row 1 should have diff_days = 14, and 90-day indicator = 1, 30-day = 0\n",
    "    assert results[0][\"diff_days\"] == 14\n",
    "    assert results[0][\"READMISSION_90_DAY_IND\"] == 1\n",
    "    assert results[0][\"READMISSION_30_DAY_IND\"] == 0\n",
    "    \n",
    "    # Row 2 should have diff_days = 76 (from Jan 15 to Apr 1) > 30, so 30-day = 0, 90-day = 1\n",
    "    assert results[1][\"diff_days\"] == 76\n",
    "    assert results[1][\"READMISSION_90_DAY_IND\"] == 1\n",
    "    assert results[1][\"READMISSION_30_DAY_IND\"] == 0\n",
    "    \n",
    "    # Row 3 (last encounter) has no next encounter so diff_days is null, readmission indicators = 0\n",
    "    assert results[2][\"diff_days\"] is None\n",
    "    assert results[2][\"READMISSION_90_DAY_IND\"] == 0\n",
    "    assert results[2][\"READMISSION_30_DAY_IND\"] == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7db4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_opioid_indicator(spark):\n",
    "    # Create a sample medications DataFrame\n",
    "    data = [\n",
    "        (\"patient_1\", \"2022-01-01 08:00:00\", \"MED001\"),  # assume MED001 is restricted\n",
    "        (\"patient_1\", \"2022-01-01 08:05:00\", \"MED002\"),  # not restricted\n",
    "        (\"patient_2\", \"2022-01-01 09:00:00\", \"MED003\")   # assume MED003 is restricted\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"PATIENT\", StringType(), True),\n",
    "        StructField(\"enc_START\", StringType(), True),\n",
    "        StructField(\"CODE\", StringType(), True)\n",
    "    ])\n",
    "    med_df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "    # Define your list of restricted opioid codes\n",
    "    restricted_codes = [\"MED001\", \"MED003\"]\n",
    "    \n",
    "    # Add CURRENT_OPIOID_IND column\n",
    "    med_df = med_df.withColumn(\n",
    "        \"CURRENT_OPIOID_IND\",\n",
    "        Func.when(Func.col(\"CODE\").isin(*restricted_codes), Func.lit(1)).otherwise(Func.lit(0))\n",
    "    )\n",
    "    \n",
    "    results = med_df.collect()\n",
    "    \n",
    "    # Assert that patients with MED001 and MED003 get flagged appropriately\n",
    "    for row in results:\n",
    "        if row[\"CODE\"] in restricted_codes:\n",
    "            assert row[\"CURRENT_OPIOID_IND\"] == 1\n",
    "        else:\n",
    "            assert row[\"CURRENT_OPIOID_IND\"] == 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
