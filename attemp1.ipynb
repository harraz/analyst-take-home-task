{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9550702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc784194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 13:17:38 WARN Utils: Your hostname, debian-shed resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface enp3s0)\n",
      "25/04/11 13:17:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 13:17:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: allergies.csv\n",
      "Processing file: encounters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: medications.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: patients.csv\n",
      "Processing file: procedures.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ModularCSVLoader\").getOrCreate()\n",
    "\n",
    "# Base URL for all the CSV files\n",
    "base_url = \"datasets/\"\n",
    "\n",
    "# List of file names to process\n",
    "file_names = [\n",
    "\"allergies.csv\",\n",
    "\"encounters.csv\",\n",
    "\"medications.csv\",\n",
    "\"patients.csv\",\n",
    "\"procedures.csv\"\n",
    "]\n",
    "\n",
    "# Dictionaries to store the resulting DataFrames for further processing/joining.\n",
    "# pandas_dfs = {}\n",
    "spark_dfs = {}\n",
    "\n",
    "# Process each file and store the DataFrames into the dictionaries\n",
    "for file_name in file_names:\n",
    "    name_key = file_name.replace('.csv', '')\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    spark_df = spark.read.csv(file_url, header=True, inferSchema=True)\n",
    "    # pandas_dfs[name_key] = pd_df\n",
    "    spark_dfs[name_key] = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f47d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+--------------------+--------------+--------+--------------------+------+----------+-----------------+\n",
      "|                  Id|              START|               STOP|             PATIENT|            PROVIDER|ENCOUNTERCLASS|    CODE|         DESCRIPTION|  COST|REASONCODE|REASONDESCRIPTION|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+--------------+--------+--------------------+------+----------+-----------------+\n",
      "|2a917920-2701-49f...|2003-03-31 21:50:51|2003-04-08 13:20:43|708b81c9-21a9-411...|fb37c581-84a6-351...|     emergency|50849002|Emergency Room Ad...|105.37|  55680006|    Drug overdose|\n",
      "|22874b3d-0873-40e...|2012-02-18 21:50:51|2012-02-28 21:12:17|708b81c9-21a9-411...|fb37c581-84a6-351...|     emergency|50849002|Emergency Room Ad...|105.37|  55680006|    Drug overdose|\n",
      "|134c5ee3-1b72-4e3...|2013-08-03 21:50:51|2013-08-13 07:44:52|708b81c9-21a9-411...|fb37c581-84a6-351...|     emergency|50849002|Emergency Room Ad...|105.37|  55680006|    Drug overdose|\n",
      "|6125f147-72d4-48a...|2014-12-08 21:50:51|2014-12-17 12:25:27|708b81c9-21a9-411...|fb37c581-84a6-351...|     emergency|50849002|Emergency Room Ad...|105.37|  55680006|    Drug overdose|\n",
      "|f837dcf8-af7d-43b...|2015-08-31 21:50:51|2015-09-08 12:04:08|708b81c9-21a9-411...|fb37c581-84a6-351...|     emergency|50849002|Emergency Room Ad...|105.37|  55680006|    Drug overdose|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+--------------+--------+--------------------+------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "enc_df = spark_dfs['encounters'].filter(\n",
    "    (spark_dfs['encounters'].REASONCODE == '55680006') &\n",
    "    (spark_dfs['encounters'].START > F.lit(\"1999-07-15 00:00:00\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "enc_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10012af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+-----------+---------+----------+------+-----------+----------+------+------------+-------+-----+------------+------+--------------------+--------------------+-------------+------------+-----+\n",
      "|                  Id| BIRTHDATE|DEATHDATE|        SSN|  DRIVERS|  PASSPORT|PREFIX|      FIRST|      LAST|SUFFIX|      MAIDEN|MARITAL| RACE|   ETHNICITY|GENDER|          BIRTHPLACE|             ADDRESS|         CITY|       STATE|  ZIP|\n",
      "+--------------------+----------+---------+-----------+---------+----------+------+-----------+----------+------+------------+-------+-----+------------+------+--------------------+--------------------+-------------+------------+-----+\n",
      "|3d8e57b2-3de5-4fb...|1943-03-11|       NA|999-86-7250|S99939389| X3970685X|  Mrs.|   Allyn942|Kreiger457|    NA|Bartoletti50|      M|asian|asian_indian|     F|Muhlenberg  Penns...|372 Marks Heights...|Middle Paxton|Pennsylvania|   NA|\n",
      "|7f4ea9fb-f436-411...|1980-09-28|       NA|999-90-4314|S99920355|X28211313X|   Mr.|   Kieth891|   King743|    NA|          NA|      M|white|       irish|     M|Overfield  Pennsy...|428 Strosin Fort ...|     Limerick|Pennsylvania|   NA|\n",
      "|553b00b2-347c-48e...|1973-02-22|       NA|999-78-9189|S99980820| X9988931X|  Mrs.|Domenica436|  Fadel536|    NA|  Labadie908|      M|white|    scottish|     F|Wilkins  Pennsylv...|   519 Ziemann Trail|   Washington|Pennsylvania|15301|\n",
      "|50f799aa-740c-4da...|1955-03-30|       NA|999-49-5162|S99961309|X17464073X|   Mr.|   Kelly223| Turner526|    NA|          NA|      M|white|       irish|     M|Manor  Pennsylvan...|158 Dickens Corne...|      Manheim|Pennsylvania|17545|\n",
      "|bcef3b7a-0380-4b7...|2009-05-24|       NA|999-74-7736|       NA|        NA|    NA|    Zack583|    Purdy2|    NA|          NA|     NA|white|      german|     M|Cranberry  Pennsy...| 1000 Vandervort Run|   Washington|Pennsylvania|15301|\n",
      "+--------------------+----------+---------+-----------+---------+----------+------+-----------+----------+------+------------+-------+-----+------------+------+--------------------+--------------------+-------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df = spark_dfs['patients'].filter(\n",
    "    (spark_dfs['patients'].BIRTHDATE.isNotNull())\n",
    ")\n",
    "\n",
    "patient_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bd6837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+---------+--------------------+-------------------+-------------------+---+\n",
      "|              pat_Id|              enc_Id| BIRTHDATE|DEATHDATE|      enc_PATIENT_id|          enc_START|           enc_STOP|age|\n",
      "+--------------------+--------------------+----------+---------+--------------------+-------------------+-------------------+---+\n",
      "|708b81c9-21a9-411...|22874b3d-0873-40e...|1986-10-28|       NA|708b81c9-21a9-411...|2012-02-18 21:50:51|2012-02-28 21:12:17| 25|\n",
      "|708b81c9-21a9-411...|134c5ee3-1b72-4e3...|1986-10-28|       NA|708b81c9-21a9-411...|2013-08-03 21:50:51|2013-08-13 07:44:52| 26|\n",
      "|708b81c9-21a9-411...|6125f147-72d4-48a...|1986-10-28|       NA|708b81c9-21a9-411...|2014-12-08 21:50:51|2014-12-17 12:25:27| 28|\n",
      "|708b81c9-21a9-411...|f837dcf8-af7d-43b...|1986-10-28|       NA|708b81c9-21a9-411...|2015-08-31 21:50:51|2015-09-08 12:04:08| 28|\n",
      "|65b093e4-b353-447...|010594a6-a6ff-487...|1995-12-03|       NA|65b093e4-b353-447...|2018-12-10 00:59:35|2018-12-20 22:57:39| 23|\n",
      "+--------------------+--------------------+----------+---------+--------------------+-------------------+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "pat = patient_df.alias(\"pat\")\n",
    "enc = enc_df.alias(\"enc\")\n",
    "\n",
    "# Join on patient Id. Use proper aliases when referring to columns.\n",
    "joined_df = pat.join(enc, pat[\"Id\"] == enc[\"PATIENT\"], \"inner\")\n",
    "\n",
    "# Select the columns of interest from each DataFrame and rename encounter columns to avoid ambiguity.\n",
    "joined_df = joined_df.select(\n",
    "    pat[\"Id\"].alias(\"pat_Id\"), enc['id'].alias(\"enc_Id\"),\n",
    "    pat[\"BIRTHDATE\"], pat['DEATHDATE'],\n",
    "    enc[\"PATIENT\"].alias(\"enc_PATIENT_id\"),\n",
    "    enc[\"START\"].alias(\"enc_START\"),\n",
    "    enc[\"STOP\"].alias(\"enc_STOP\")\n",
    ")\n",
    "\n",
    "# Calculate the patient's age at the time of the encounter.\n",
    "# Assuming \"BIRTHDATE\" is in a format recognized by Spark for date conversion.\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"age\",\n",
    "    F.floor(F.datediff(F.col(\"enc_START\").cast(\"date\"), F.col(\"BIRTHDATE\").cast(\"date\")) / 365)\n",
    ")\n",
    "\n",
    "# Filter the records where age is between 18 and 35\n",
    "cohort_df = joined_df.filter((F.col(\"age\") >= 18) & (F.col(\"age\") <= 35))\n",
    "\n",
    "cohort_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3eab1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|DEATHDATE|DEATH_AT_VISIT_IND|\n",
      "+---------+------------------+\n",
      "|NA       |0                 |\n",
      "|NA       |0                 |\n",
      "|NA       |0                 |\n",
      "|NA       |0                 |\n",
      "|NA       |0                 |\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cohort_df = cohort_df.withColumn(\n",
    "    \"DEATH_AT_VISIT_IND\",\n",
    "    Func.when(\n",
    "        # Check that DEATHDATE is not \"NA\" and falls between the START and STOP dates:\n",
    "        (Func.col(\"DEATHDATE\") != \"NA\") &\n",
    "        (Func.to_timestamp(Func.col(\"DEATHDATE\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\").between(Func.col('enc_START').cast(\"timestamp\"), Func.col('enc_STOP').cast(\"timestamp\"))),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# For debugging or previewing results:\n",
    "cohort_df.select(\"DEATHDATE\", \"DEATH_AT_VISIT_IND\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af99f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+---+------------------+\n",
      "|pat_Id                              |enc_Id                              |BIRTHDATE |DEATHDATE |enc_PATIENT_id                      |enc_START          |enc_STOP           |age|DEATH_AT_VISIT_IND|\n",
      "+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+---+------------------+\n",
      "|dacea80b-75dd-42d6-a5c0-be369c3e4ebf|1deb51e9-ef0b-4013-b46f-f8efcd836842|1979-12-11|2009-08-24|dacea80b-75dd-42d6-a5c0-be369c3e4ebf|2009-08-20 01:30:34|2009-08-24 21:37:59|29 |1                 |\n",
      "|978114f9-f9f2-4361-b64d-8045ab8f1602|41c9fbf6-a7d9-48a1-93fc-9fa32d554083|1976-01-07|2009-09-01|978114f9-f9f2-4361-b64d-8045ab8f1602|2009-08-22 16:34:32|2009-09-01 17:22:53|33 |1                 |\n",
      "|7cdbf215-8dce-4641-ae53-ea0f9a4c84f0|673a3777-537e-4122-ae94-63d9349f42c6|1987-04-11|2017-07-24|7cdbf215-8dce-4641-ae53-ea0f9a4c84f0|2017-07-18 20:57:47|2017-07-24 16:22:05|30 |1                 |\n",
      "|5fa3711b-4012-4c34-aaf8-0097d0785418|eef21ec6-4938-4a57-a782-ba5b38efabe2|1985-04-19|2013-10-20|5fa3711b-4012-4c34-aaf8-0097d0785418|2013-10-14 00:45:01|2013-10-20 01:59:20|28 |1                 |\n",
      "|443dabde-8301-451f-af6a-da316e3b96a7|3a01a5cb-d7d8-450b-ad2d-db120fc9cf27|1985-11-06|2015-08-30|443dabde-8301-451f-af6a-da316e3b96a7|2015-08-25 10:19:41|2015-08-30 23:32:44|29 |1                 |\n",
      "+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "cohort_df = cohort_df.filter(Func.col(\"DEATH_AT_VISIT_IND\") == 1)\n",
    "\n",
    "# Show the filtered records\n",
    "cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------------+------------------------------------+-------+-------------------------------------+-------+---------+---------+----------+-----------------+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+---+------------------+\n",
      "|med_START |med_STOP  |med_PATIENT                         |ENCOUNTER                           |CODE   |DESCRIPTION                          |COST   |DISPENSES|TOTALCOST|REASONCODE|REASONDESCRIPTION|pat_Id                              |enc_Id                              |BIRTHDATE |DEATHDATE |enc_PATIENT_id                      |enc_START          |enc_STOP           |age|DEATH_AT_VISIT_IND|\n",
      "+----------+----------+------------------------------------+------------------------------------+-------+-------------------------------------+-------+---------+---------+----------+-----------------+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+---+------------------+\n",
      "|2016-07-20|2016-08-10|04aefd77-78d8-4590-a7fb-92b9a49df0f1|61930df2-3874-41e7-9d4b-d208808e9a18|835900 |cycloSPORINE 50 MG Oral Capsule      |311.51 |1        |311.51   |24079001  |Atopic dermatitis|04aefd77-78d8-4590-a7fb-92b9a49df0f1|c706c531-230b-41ad-8d7f-93f8a958540e|1980-08-05|2013-09-17|04aefd77-78d8-4590-a7fb-92b9a49df0f1|2013-09-11 20:07:38|2013-09-17 19:09:05|33 |1                 |\n",
      "|2016-07-20|2016-08-10|04aefd77-78d8-4590-a7fb-92b9a49df0f1|61930df2-3874-41e7-9d4b-d208808e9a18|106258 |Hydrocortisone 10 MG/ML Topical Cream|4.75   |1        |4.75     |24079001  |Atopic dermatitis|04aefd77-78d8-4590-a7fb-92b9a49df0f1|c706c531-230b-41ad-8d7f-93f8a958540e|1980-08-05|2013-09-17|04aefd77-78d8-4590-a7fb-92b9a49df0f1|2013-09-11 20:07:38|2013-09-17 19:09:05|33 |1                 |\n",
      "|2017-03-16|2018-03-11|63b83406-e59e-456c-9a38-69fa768913be|3e0f1471-09e0-418c-b5cb-e70f6cf97ec9|748856 |Yaz 28 Day Pack                      |34.82  |12       |417.84   |NA        |NA               |63b83406-e59e-456c-9a38-69fa768913be|b2c9eb0c-2937-42ba-b116-6b5d0bdf4576|1996-09-19|2016-06-16|63b83406-e59e-456c-9a38-69fa768913be|2016-06-10 16:10:40|2016-06-16 00:19:14|19 |1                 |\n",
      "|2017-12-14|2017-12-14|63b83406-e59e-456c-9a38-69fa768913be|74352b14-0022-4943-a10f-8c488f699e18|309362 |Clopidogrel 75 MG Oral Tablet        |90.33  |1        |90.33    |NA        |NA               |63b83406-e59e-456c-9a38-69fa768913be|b2c9eb0c-2937-42ba-b116-6b5d0bdf4576|1996-09-19|2016-06-16|63b83406-e59e-456c-9a38-69fa768913be|2016-06-10 16:10:40|2016-06-16 00:19:14|19 |1                 |\n",
      "|2017-12-14|2017-12-14|63b83406-e59e-456c-9a38-69fa768913be|74352b14-0022-4943-a10f-8c488f699e18|1804799|Alteplase 100 MG Injection           |5355.52|1        |5355.52  |NA        |NA               |63b83406-e59e-456c-9a38-69fa768913be|b2c9eb0c-2937-42ba-b116-6b5d0bdf4576|1996-09-19|2016-06-16|63b83406-e59e-456c-9a38-69fa768913be|2016-06-10 16:10:40|2016-06-16 00:19:14|19 |1                 |\n",
      "+----------+----------+------------------------------------+------------------------------------+-------+-------------------------------------+-------+---------+---------+----------+-----------------+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# COUNT_CURRENT_MEDS: Count of active medications at the start of the drug overdose encounter\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "med = spark_dfs['medications'].alias(\"m\")\n",
    "cohort = cohort_df.alias(\"c\")\n",
    "\n",
    "# Join the DataFrames using their aliases for clarity in the join condition\n",
    "med_df = med.join(cohort, F.col(\"c.pat_id\") == F.col(\"m.PATIENT\"), \"inner\") \\\n",
    "    .filter(\n",
    "        F.col(\"m.START\").cast(\"timestamp\") >= F.col(\"c.enc_START\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Now use the actual column names for renaming; after join, med.PATIENT will appear as \"PATIENT\"\n",
    "med_df = med_df.withColumnRenamed(\"PATIENT\", \"med_PATIENT\") \\\n",
    "    .withColumnRenamed(\"START\", \"med_START\") \\\n",
    "    .withColumnRenamed(\"STOP\", \"med_STOP\") \n",
    "\n",
    "\n",
    "med_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab39a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+---------------+\n",
      "|med_PATIENT                         |ENCOUNTER                           |total_med_count|\n",
      "+------------------------------------+------------------------------------+---------------+\n",
      "|124bb400-58b3-4352-acaa-f79c84397795|6582cf77-7929-4b34-97ff-88b7cd11f49c|1              |\n",
      "|5fa3711b-4012-4c34-aaf8-0097d0785418|68988987-ad1e-40c3-8c7d-5fb7e099480b|1              |\n",
      "|a20356aa-08cd-4c33-9667-08ffd3130676|23bffe9b-7d1e-4985-9260-c860c4b72315|2              |\n",
      "|77b4cadf-2eb6-4019-96f0-d4a8b344401d|e723d4a7-a71d-4951-bdc6-617c9b19bf55|1              |\n",
      "|dacea80b-75dd-42d6-a5c0-be369c3e4ebf|c3c43259-21c7-4bbc-accc-011c50a0ef77|1              |\n",
      "|28202667-2977-4d2f-ba68-977a98827104|cb13d070-c2b3-4ac6-9920-0d89c216442a|1              |\n",
      "|0ea047ce-faa7-4320-be2f-672a93fc9cfb|868c907d-4892-4771-a509-4e0f85015a4d|1              |\n",
      "|28202667-2977-4d2f-ba68-977a98827104|23ae1d57-58e7-4cbb-b51b-171d03d46041|1              |\n",
      "|dacea80b-75dd-42d6-a5c0-be369c3e4ebf|3dd20f27-2113-489d-81a2-6de84bf0029a|1              |\n",
      "|77b4cadf-2eb6-4019-96f0-d4a8b344401d|7edc630d-0e30-4f38-9ad1-080e4d0a1195|2              |\n",
      "|0ea047ce-faa7-4320-be2f-672a93fc9cfb|f42e178a-cd3b-448f-a385-d921c2e2bf64|1              |\n",
      "|417767cc-c235-4ba2-9b75-c519056ba684|76d228f8-bfba-4894-8399-7540b0fd2a09|1              |\n",
      "|dacea80b-75dd-42d6-a5c0-be369c3e4ebf|8eb5ad3c-8952-4162-a93d-e2dbf8c8bb9e|1              |\n",
      "|28202667-2977-4d2f-ba68-977a98827104|955fccd2-5b95-4d7e-a70d-578d658d4782|1              |\n",
      "|124bb400-58b3-4352-acaa-f79c84397795|7f0b5289-4f25-4d96-8718-44fb3010dcca|1              |\n",
      "|bc710014-bbb9-45a3-83a5-8e915268012e|8269883c-8d0e-4e6d-bdf0-3d3199bcfafa|1              |\n",
      "|864a2889-c655-4a18-aacf-785faa2e5f8d|49642a8b-0583-4c60-86e8-459259f5be4f|1              |\n",
      "|443dabde-8301-451f-af6a-da316e3b96a7|c8d4f501-48e0-4d2b-94c4-5c444a773f96|1              |\n",
      "|bc710014-bbb9-45a3-83a5-8e915268012e|60a76d45-9e81-46bf-afd7-466cd7217910|1              |\n",
      "|63b83406-e59e-456c-9a38-69fa768913be|74352b14-0022-4943-a10f-8c488f699e18|2              |\n",
      "+------------------------------------+------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_med = med_df.groupBy(\n",
    "    \"CODE\", \"ENCOUNTER\", \"med_PATIENT\"\n",
    ").agg(F.count(\"*\").alias(\"med_cnt\"))\n",
    "\n",
    "grouped_med = grouped_med.groupBy(\"med_PATIENT\", \"ENCOUNTER\").agg(\n",
    "    F.sum(\"med_cnt\").alias(\"total_med_count\")\n",
    ")\n",
    "\n",
    "grouped_med.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f20e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = patient_df.count()\n",
    "\n",
    "# Calculate the number of nulls for each column\n",
    "null_counts = patient_df.select(\n",
    "    [(Func.count(Func.when(Func.col(c).isNull(), c)) / total_rows).alias(c) for c in patient_df.columns]\n",
    ").collect()[0]\n",
    "\n",
    "# Convert the null counts to a dictionary\n",
    "null_counts_dict = {col: null_counts[i] for i, col in enumerate(patient_df.columns)}\n",
    "\n",
    "# Plot the null percentages\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(null_counts_dict.keys(), null_counts_dict.values(), color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage of Null Values')\n",
    "plt.title('Null Value Percentage by Column in patient_df')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
