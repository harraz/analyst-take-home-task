{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9550702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc784194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 18:36:58 WARN Utils: Your hostname, debian-shed resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface enp3s0)\n",
      "25/04/11 18:36:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 18:36:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/11 18:36:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: allergies.csv\n",
      "Processing file: encounters.csv\n",
      "Processing file: encounters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: medications.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: patients.csv\n",
      "Processing file: procedures.csv\n",
      "Processing file: procedures.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"ModularCSVLoader\").getOrCreate()\n",
    "\n",
    "# Base URL for all the CSV files\n",
    "base_url = \"datasets/\"\n",
    "\n",
    "# List of file names to process\n",
    "file_names = [\n",
    "\"allergies.csv\",\n",
    "\"encounters.csv\",\n",
    "\"medications.csv\",\n",
    "\"patients.csv\",\n",
    "\"procedures.csv\"\n",
    "]\n",
    "\n",
    "# Dictionaries to store the resulting DataFrames for further processing/joining.\n",
    "spark_dfs = {}\n",
    "\n",
    "# Process each file and store the DataFrames in a dictiona\n",
    "for file_name in file_names:\n",
    "    name_key = file_name.replace('.csv', '')\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    spark_df = spark.read.csv(file_url, header=True, inferSchema=True)\n",
    "    spark_dfs[name_key] = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f47d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Assemble the project cohort\n",
    "# Filter the 'encounters' DataFrame for specific conditions (e.g., REASONCODE and START date)\n",
    "\n",
    "enc_df = spark_dfs['encounters'].filter(\n",
    "    (spark_dfs['encounters'].REASONCODE == '55680006') &\n",
    "    (spark_dfs['encounters'].START > Func.lit(\"1999-07-15 00:00:00\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# enc_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10012af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a patients dataframe with only the patients that have a birthdate\n",
    "patient_df = spark_dfs['patients'].filter(\n",
    "    (spark_dfs['patients'].BIRTHDATE.isNotNull())\n",
    ")\n",
    "\n",
    "# patient_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bd6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 1: Assemble the project cohort\n",
    "The cohort is defined as patients aged 18-35 at the time of the encounter with a specific reason code.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "pat = patient_df.alias(\"pat\")\n",
    "enc = enc_df.alias(\"enc\")\n",
    "\n",
    "# Join on patient Id. Use proper aliases when referring to columns.\n",
    "joined_df = pat.join(enc, pat[\"Id\"] == enc[\"PATIENT\"], \"inner\")\n",
    "\n",
    "# Select and rename same name columns to avoid ambiguity.\n",
    "joined_df = joined_df.select(\n",
    "    pat[\"Id\"].alias(\"pat_Id\"), enc['id'].alias(\"enc_Id\"),\n",
    "    pat[\"BIRTHDATE\"], pat['DEATHDATE'],\n",
    "    enc[\"PATIENT\"].alias(\"enc_PATIENT_id\"),\n",
    "    enc[\"START\"].alias(\"enc_START\"),\n",
    "    enc[\"STOP\"].alias(\"enc_STOP\")\n",
    ")\n",
    "\n",
    "# Calculate the patient's age at the time of the encounter.\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"AGE_AT_VISIT\",\n",
    "    Func.floor(Func.datediff(Func.col(\"enc_START\").cast(\"date\"), Func.col(\"BIRTHDATE\").cast(\"date\")) / 365)\n",
    ")\n",
    "\n",
    "# Filter the records where age is between 18 and 35\n",
    "cohort_df = joined_df.filter((Func.col(\"AGE_AT_VISIT\") >= 18) & (Func.col(\"AGE_AT_VISIT\") <= 35))\n",
    "\n",
    "# cohort_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3eab1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "DEATH_AT_VISIT_IND: 1 if patient died during the drug overdose encounter, 0 if the patient died at a different time\n",
    "This cell adds a new column to the cohort DataFrame indicating whether the patient died during the encounter.\n",
    "\"\"\"\n",
    "\n",
    "cohort_df = cohort_df.withColumn(\n",
    "    \"DEATH_AT_VISIT_IND\",\n",
    "    Func.when(\n",
    "        # Check that DEATHDATE is not \"NA\" and falls between the START and STOP dates:\n",
    "        (Func.col(\"DEATHDATE\") != \"NA\") &\n",
    "        (Func.to_timestamp(Func.col(\"DEATHDATE\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\").between(Func.col('enc_START').cast(\"timestamp\"), Func.col('enc_STOP').cast(\"timestamp\"))),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# For debugging or previewing results:\n",
    "# cohort_df.select(\"DEATHDATE\", \"DEATH_AT_VISIT_IND\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af99f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes\n",
    "\n",
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "# test_cohort_df = cohort_df.filter(Func.col(\"DEATH_AT_VISIT_IND\") == 1)\n",
    "\n",
    "# # Show the filtered records\n",
    "# test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04efbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Create additional fields\n",
    "\n",
    "\"\"\"\n",
    "This cell counts the number of active medications at the start of the drug overdose encounter.\n",
    "It joins the medications DataFrame with the cohort DataFrame on patient ID and filters based on the encounter start date.\n",
    "The result is a DataFrame with the count of current medications for each patient at the time of the encounter.\n",
    "\"\"\"\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "med = spark_dfs['medications'].alias(\"m\")\n",
    "cohort = cohort_df.alias(\"c\")\n",
    "\n",
    "# Join the DataFrames using their aliases for clarity in the join condition\n",
    "med_df = med.join(cohort, Func.col(\"c.pat_id\") == Func.col(\"m.PATIENT\"), \"inner\") \\\n",
    "    .filter(\n",
    "        Func.col(\"m.START\").cast(\"timestamp\") >= Func.col(\"c.enc_START\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Now use the actual column names for renaming; after join, med.PATIENT will appear as \"PATIENT\"\n",
    "med_df = med_df.withColumnRenamed(\"PATIENT\", \"med_PATIENT\") \\\n",
    "    .withColumnRenamed(\"START\", \"med_START\") \\\n",
    "    .withColumnRenamed(\"STOP\", \"med_STOP\") \n",
    "\n",
    "grouped_med = med_df.groupBy(\n",
    "    \"CODE\", \"ENCOUNTER\", \"med_PATIENT\"\n",
    ").agg(Func.count(\"*\").alias(\"med_cnt\"))\n",
    "\n",
    "grouped_med = grouped_med.groupBy(\"med_PATIENT\", \"ENCOUNTER\").agg(\n",
    "    Func.sum(\"med_cnt\").alias(\"COUNT_CURRENT_MEDS\")\n",
    ")\n",
    "\n",
    "med_df = med_df.join(grouped_med,\n",
    "    (med_df[\"med_PATIENT\"] == grouped_med[\"med_PATIENT\"]) &\n",
    "    (med_df[\"ENCOUNTER\"] == grouped_med[\"ENCOUNTER\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    med_df[\"*\"],\n",
    "    grouped_med[\"COUNT_CURRENT_MEDS\"]\n",
    ")\n",
    "\n",
    "# cohor and med df are now mergerd\n",
    "# med_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97cf0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316049, 429503]\n"
     ]
    }
   ],
   "source": [
    "# Calcuale and add CURRENT_OPIOID_IND\t\n",
    "# if the patient had at least one active medication at the start of the overdose encounter that is on the Opioids List (provided below)\n",
    "\n",
    "# GET CODES FOR  Opioids List:\n",
    "# Hydromorphone 325Mg\n",
    "# Fentanyl – 100 MCG\n",
    "# Oxycodone-acetaminophen 100 Ml\n",
    "\n",
    "# Define the list of opioid patterns to search for in the DESCRIPTION column\n",
    "patterns = [\n",
    "    \"(?i)^Hydromorphone 325\", \n",
    "    \"(?i)^Fentanyl\", \n",
    "    \"(?i)^Oxycodone-acetaminophen 100\"\n",
    "]\n",
    "\n",
    "# Build a filter condition by OR-ing each pattern on the DESCRIPTION column\n",
    "filter_condition = None\n",
    "for pattern in patterns:\n",
    "    cond = Func.col(\"DESCRIPTION\").rlike(pattern)\n",
    "    filter_condition = cond if filter_condition is None else filter_condition | cond\n",
    "\n",
    "# Filter med_df using the combined condition and return only the distinct CODE column.\n",
    "# Then extract the CODE values as a Python list.\n",
    "opioid_codes_list = [row[\"CODE\"] for row in med_df.filter(filter_condition).select(\"CODE\").distinct().collect()]\n",
    "\n",
    "# Print the resulting list of opioid codes\n",
    "print(opioid_codes_list)\n",
    "\n",
    "# Add the CURRENT_OPIOID_IND column: 1 if med_df.CODE is in restricted_codes_list, else 0.\n",
    "cohort_df = med_df.withColumn(\n",
    "    \"CURRENT_OPIOID_IND\",\n",
    "    Func.when(Func.col(\"CODE\").isin(*opioid_codes_list), Func.lit(1)).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "# cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------------+------------------------------------+------+--------------------+------+---------+---------+----------+-----------------+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+------------+------------------+------------------+------------------+\n",
      "|med_START |med_STOP  |med_PATIENT                         |ENCOUNTER                           |CODE  |DESCRIPTION         |COST  |DISPENSES|TOTALCOST|REASONCODE|REASONDESCRIPTION|pat_Id                              |enc_Id                              |BIRTHDATE |DEATHDATE |enc_PATIENT_id                      |enc_START          |enc_STOP           |AGE_AT_VISIT|DEATH_AT_VISIT_IND|COUNT_CURRENT_MEDS|CURRENT_OPIOID_IND|\n",
      "+----------+----------+------------------------------------+------------------------------------+------+--------------------+------+---------+---------+----------+-----------------+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+------------+------------------+------------------+------------------+\n",
      "|2011-03-10|2011-04-09|ebe51a1a-1447-4bb5-a638-ac0163db989b|21f7138b-2d47-48a9-b243-c29c0e229c2d|429503|Fentanyl 100 MCG    |214.96|1        |214.96   |59621000  |Pain Relief      |ebe51a1a-1447-4bb5-a638-ac0163db989b|bbe83c18-5874-425f-928f-77cc7d0c74c6|1992-08-06|NA        |ebe51a1a-1447-4bb5-a638-ac0163db989b|2010-12-16 08:23:35|2010-12-28 07:27:19|18          |0                 |1                 |1                 |\n",
      "|2010-07-07|NA        |0ea047ce-faa7-4320-be2f-672a93fc9cfb|59114970-f6e0-4a21-b0e7-232b1a1e1518|316049|Hydromorphone 325 MG|214.96|109      |23430.64 |59621000  |Pain Relief      |0ea047ce-faa7-4320-be2f-672a93fc9cfb|f384861a-7926-4cf3-9f06-8b2b28836371|1991-07-17|2011-06-06|0ea047ce-faa7-4320-be2f-672a93fc9cfb|2009-12-07 20:01:53|2009-12-16 13:51:53|18          |0                 |1                 |1                 |\n",
      "|2010-01-18|NA        |dbfec58c-64a9-4fe0-b095-64b03af7fd67|8a6011fa-9c60-4661-aa3b-32066e5b76c3|316049|Hydromorphone 325 MG|214.96|115      |24720.4  |59621000  |Pain Relief      |dbfec58c-64a9-4fe0-b095-64b03af7fd67|5664eb64-9ce1-4bdf-9706-469cc4bf2d67|1978-11-06|2001-02-24|dbfec58c-64a9-4fe0-b095-64b03af7fd67|2001-02-18 23:17:27|2001-02-24 18:53:41|22          |1                 |1                 |1                 |\n",
      "|2018-07-12|2018-08-11|3b3b7748-a006-466d-9788-b3d6e922ffc4|a6c595a6-a125-4c60-a813-c6dbb92c7f14|429503|Fentanyl 100 MCG    |214.96|1        |214.96   |59621000  |Pain Relief      |3b3b7748-a006-466d-9788-b3d6e922ffc4|b9a23377-8962-4da1-b6fc-bbf8581f895e|1976-11-25|2010-02-17|3b3b7748-a006-466d-9788-b3d6e922ffc4|2002-09-19 03:17:53|2002-09-27 00:48:54|25          |0                 |2                 |1                 |\n",
      "|2018-07-12|2018-08-11|3b3b7748-a006-466d-9788-b3d6e922ffc4|a6c595a6-a125-4c60-a813-c6dbb92c7f14|429503|Fentanyl 100 MCG    |214.96|1        |214.96   |59621000  |Pain Relief      |3b3b7748-a006-466d-9788-b3d6e922ffc4|9fcf2e25-4c60-45c6-a10a-55ef1415569d|1976-11-25|2010-02-17|3b3b7748-a006-466d-9788-b3d6e922ffc4|2010-02-08 03:17:53|2010-02-17 02:46:40|33          |1                 |2                 |1                 |\n",
      "+----------+----------+------------------------------------+------------------------------------+------+--------------------+------+---------+---------+----------+-----------------+------------------------------------+------------------------------------+----------+----------+------------------------------------+-------------------+-------------------+------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use for debuigging and testing purposes\n",
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "# test_cohort_df = cohort_df.filter(Func.col(\"CURRENT_OPIOID_IND\") == 1)\n",
    "\n",
    "# # Show the filtered records\n",
    "# test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128ec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a patient-partitioned window to compute each encounter's next start date \n",
    "    and the day difference between encounters, then assigns 90-day and 30-day readmission indicators, \n",
    "    conditionally replaces the next encounter start date with \"N/A\" when appropriate, renames that column to FIRST_READMISSION_DATE, \n",
    "    and finally displays filtered results.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window partitioned by pat_Id and ordered by enc_STOP (converted to timestamp)\n",
    "patient_window = Window.partitionBy(\"pat_Id\").orderBy(Func.col(\"enc_STOP\").cast(\"timestamp\"))\n",
    "\n",
    "# Get the next encounter's start date using lead().\n",
    "df_with_next = cohort_df.withColumn(\"next_enc_START\", Func.lead(\"enc_START\").over(patient_window))\n",
    "\n",
    "# Calculate the difference in days between the current encounter and the next encounter.\n",
    "df_with_diff = df_with_next.withColumn(\"diff_days\", Func.datediff(Func.col(\"next_enc_START\"), Func.col(\"enc_START\")))\n",
    "\n",
    "# Create the READMISSION_90_DAY_IND indicator: flag as 1 if the next encounter is within 90 days and not 0, else 0.\n",
    "df_with_indicator = df_with_diff.withColumn(\n",
    "    \"READMISSION_90_DAY_IND\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\").isNotNull()) &\n",
    "        (Func.col(\"diff_days\") != 0) &\n",
    "        (Func.col(\"diff_days\") <= 90), Func.lit(1)\n",
    "    ).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "#  create a 30-day readmission indicator.\n",
    "df_with_indicator = df_with_indicator.withColumn(\n",
    "    \"READMISSION_30_DAY_IND\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\").isNotNull()) &\n",
    "        (Func.col(\"diff_days\") != 0) &\n",
    "        (Func.col(\"diff_days\") <= 30), Func.lit(1)\n",
    "    ).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "# # Debug filtered results: those with diff_days between 1 and 89.\n",
    "# df_with_indicator.filter(Func.col(\"diff_days\").between(1, 89)).show(5, truncate=False)\n",
    "# df_with_indicator.filter(Func.col(\"READMISSION_30_DAY_IND\") == 1).show(5, truncate=False)\n",
    "\n",
    "# Update next_enc_START to \"N/A\" when diff_days is not positive or exceeds 90 (i.e. otherwise set to \"N/A\").\n",
    "df_with_indicator = df_with_indicator.withColumn(\n",
    "    \"next_enc_START\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\") <= 90) & (Func.col(\"diff_days\") != 0), Func.col(\"next_enc_START\")\n",
    "    ).otherwise(Func.lit(\"N/A\"))\n",
    ")\n",
    "\n",
    "# Rename next_enc_START to FIRST_READMISSION_DATE.\n",
    "df_with_indicator = df_with_indicator.withColumnRenamed('next_enc_START', 'FIRST_READMISSION_DATE')\n",
    "\n",
    "# df_with_indicator.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5a1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for testing and debugging purposes\n",
    "# df_with_indicator.filter(F.col(\"diff_days\").between(1, 89)).show(5, truncate=False)\n",
    "# df_with_indicator.filter(F.col(\"READMISSION_30_DAY_IND\") == 1).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d343467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame written to output/df_with_indicators.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Export the data to a CSV file\n",
    "# Select and rename columns to match the required format\n",
    "output_df = df_with_indicator.select(\n",
    "    Func.col(\"pat_Id\").alias(\"PATIENT_ID\"),\n",
    "    Func.col(\"enc_Id\").alias(\"ENCOUNTER_ID\"),\n",
    "    Func.col(\"enc_START\").alias(\"HOSPITAL_ENCOUNTER_DATE\"),\n",
    "    Func.col(\"AGE_AT_VISIT\"),\n",
    "    Func.col(\"DEATH_AT_VISIT_IND\"),\n",
    "    Func.col(\"COUNT_CURRENT_MEDS\"),\n",
    "    Func.col(\"CURRENT_OPIOID_IND\"),\n",
    "    Func.col(\"READMISSION_90_DAY_IND\"),\n",
    "    Func.col(\"READMISSION_30_DAY_IND\"),\n",
    "    Func.col(\"FIRST_READMISSION_DATE\")\n",
    ")\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "output_path = \"output/df_with_indicators.csv\"\n",
    "output_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"DataFrame written to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
