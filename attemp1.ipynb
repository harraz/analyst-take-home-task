{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9550702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc784194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"ModularCSVLoader\").getOrCreate()\n",
    "\n",
    "# Base URL for all the CSV files\n",
    "base_url = \"datasets/\"\n",
    "\n",
    "# List of file names to process\n",
    "file_names = [\n",
    "\"allergies.csv\",\n",
    "\"encounters.csv\",\n",
    "\"medications.csv\",\n",
    "\"patients.csv\",\n",
    "\"procedures.csv\"\n",
    "]\n",
    "\n",
    "# Dictionaries to store the resulting DataFrames for further processing/joining.\n",
    "# pandas_dfs = {}\n",
    "spark_dfs = {}\n",
    "\n",
    "# Process each file and store the DataFrames in a dictiona\n",
    "for file_name in file_names:\n",
    "    name_key = file_name.replace('.csv', '')\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    spark_df = spark.read.csv(file_url, header=True, inferSchema=True)\n",
    "    # pandas_dfs[name_key] = pd_df\n",
    "    spark_dfs[name_key] = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f47d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Assemble the project cohort\n",
    "# Filter the 'encounters' DataFrame for specific conditions (e.g., REASONCODE and START date)\n",
    "\n",
    "enc_df = spark_dfs['encounters'].filter(\n",
    "    (spark_dfs['encounters'].REASONCODE == '55680006') &\n",
    "    (spark_dfs['encounters'].START > Func.lit(\"1999-07-15 00:00:00\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# enc_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10012af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a patients dataframe with only the patients that have a birthdate\n",
    "patient_df = spark_dfs['patients'].filter(\n",
    "    (spark_dfs['patients'].BIRTHDATE.isNotNull())\n",
    ")\n",
    "\n",
    "# patient_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 1: Assemble the project cohort\n",
    "The cohort is defined as patients aged 18-35 at the time of the encounter with a specific reason code.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "pat = patient_df.alias(\"pat\")\n",
    "enc = enc_df.alias(\"enc\")\n",
    "\n",
    "# Join on patient Id. Use proper aliases when referring to columns.\n",
    "joined_df = pat.join(enc, pat[\"Id\"] == enc[\"PATIENT\"], \"inner\")\n",
    "\n",
    "# Select and rename same name columns to avoid ambiguity.\n",
    "joined_df = joined_df.select(\n",
    "    pat[\"Id\"].alias(\"pat_Id\"), enc['id'].alias(\"enc_Id\"),\n",
    "    pat[\"BIRTHDATE\"], pat['DEATHDATE'],\n",
    "    enc[\"PATIENT\"].alias(\"enc_PATIENT_id\"),\n",
    "    enc[\"START\"].alias(\"enc_START\"),\n",
    "    enc[\"STOP\"].alias(\"enc_STOP\")\n",
    ")\n",
    "\n",
    "# Calculate the patient's age at the time of the encounter.\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"AGE_AT_VISIT\",\n",
    "    F.floor(F.datediff(F.col(\"enc_START\").cast(\"date\"), F.col(\"BIRTHDATE\").cast(\"date\")) / 365)\n",
    ")\n",
    "\n",
    "# Filter the records where age is between 18 and 35\n",
    "cohort_df = joined_df.filter((F.col(\"AGE_AT_VISIT\") >= 18) & (F.col(\"AGE_AT_VISIT\") <= 35))\n",
    "\n",
    "cohort_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eab1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "DEATH_AT_VISIT_IND: 1 if patient died during the drug overdose encounter, 0 if the patient died at a different time\n",
    "This cell adds a new column to the cohort DataFrame indicating whether the patient died during the encounter.\n",
    "\"\"\"\n",
    "\n",
    "cohort_df = cohort_df.withColumn(\n",
    "    \"DEATH_AT_VISIT_IND\",\n",
    "    Func.when(\n",
    "        # Check that DEATHDATE is not \"NA\" and falls between the START and STOP dates:\n",
    "        (Func.col(\"DEATHDATE\") != \"NA\") &\n",
    "        (Func.to_timestamp(Func.col(\"DEATHDATE\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\").between(Func.col('enc_START').cast(\"timestamp\"), Func.col('enc_STOP').cast(\"timestamp\"))),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# For debugging or previewing results:\n",
    "# cohort_df.select(\"DEATHDATE\", \"DEATH_AT_VISIT_IND\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "test_cohort_df = cohort_df.filter(Func.col(\"DEATH_AT_VISIT_IND\") == 1)\n",
    "\n",
    "# Show the filtered records\n",
    "test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Create additional fields\n",
    "\n",
    "\"\"\"\n",
    "This cell counts the number of active medications at the start of the drug overdose encounter.\n",
    "It joins the medications DataFrame with the cohort DataFrame on patient ID and filters based on the encounter start date.\n",
    "The result is a DataFrame with the count of current medications for each patient at the time of the encounter.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "med = spark_dfs['medications'].alias(\"m\")\n",
    "cohort = cohort_df.alias(\"c\")\n",
    "\n",
    "# Join the DataFrames using their aliases for clarity in the join condition\n",
    "med_df = med.join(cohort, F.col(\"c.pat_id\") == F.col(\"m.PATIENT\"), \"inner\") \\\n",
    "    .filter(\n",
    "        F.col(\"m.START\").cast(\"timestamp\") >= F.col(\"c.enc_START\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Now use the actual column names for renaming; after join, med.PATIENT will appear as \"PATIENT\"\n",
    "med_df = med_df.withColumnRenamed(\"PATIENT\", \"med_PATIENT\") \\\n",
    "    .withColumnRenamed(\"START\", \"med_START\") \\\n",
    "    .withColumnRenamed(\"STOP\", \"med_STOP\") \n",
    "\n",
    "grouped_med = med_df.groupBy(\n",
    "    \"CODE\", \"ENCOUNTER\", \"med_PATIENT\"\n",
    ").agg(F.count(\"*\").alias(\"med_cnt\"))\n",
    "\n",
    "grouped_med = grouped_med.groupBy(\"med_PATIENT\", \"ENCOUNTER\").agg(\n",
    "    F.sum(\"med_cnt\").alias(\"COUNT_CURRENT_MEDS\")\n",
    ")\n",
    "\n",
    "med_df = med_df.join(grouped_med,\n",
    "    (med_df[\"med_PATIENT\"] == grouped_med[\"med_PATIENT\"]) &\n",
    "    (med_df[\"ENCOUNTER\"] == grouped_med[\"ENCOUNTER\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    med_df[\"*\"],\n",
    "    grouped_med[\"COUNT_CURRENT_MEDS\"]\n",
    ")\n",
    "\n",
    "# cohor and med df are now mergerd\n",
    "med_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuale and add CURRENT_OPIOID_IND\t\n",
    "# if the patient had at least one active medication at the start of the overdose encounter that is on the Opioids List (provided below)\n",
    "\n",
    "# GET CODES FOR  Opioids List:\n",
    "# Hydromorphone 325Mg\n",
    "# Fentanyl – 100 MCG\n",
    "# Oxycodone-acetaminophen 100 Ml\n",
    "\n",
    "# Define the list of opioid patterns to search for in the DESCRIPTION column\n",
    "patterns = [\n",
    "    \"(?i)^Hydromorphone 325\", \n",
    "    \"(?i)^Fentanyl\", \n",
    "    \"(?i)^Oxycodone-acetaminophen 100\"\n",
    "]\n",
    "\n",
    "# Build a filter condition by OR-ing each pattern on the DESCRIPTION column\n",
    "filter_condition = None\n",
    "for pattern in patterns:\n",
    "    cond = Func.col(\"DESCRIPTION\").rlike(pattern)\n",
    "    filter_condition = cond if filter_condition is None else filter_condition | cond\n",
    "\n",
    "# Filter med_df using the combined condition and return only the distinct CODE column.\n",
    "# Then extract the CODE values as a Python list.\n",
    "opioid_codes_list = [row[\"CODE\"] for row in med_df.filter(filter_condition).select(\"CODE\").distinct().collect()]\n",
    "\n",
    "# Print the resulting list of opioid codes\n",
    "print(opioid_codes_list)\n",
    "\n",
    "# Add the CURRENT_OPIOID_IND column: 1 if med_df.CODE is in restricted_codes_list, else 0.\n",
    "cohort_df = med_df.withColumn(\n",
    "    \"CURRENT_OPIOID_IND\",\n",
    "    Func.when(Func.col(\"CODE\").isin(*opioid_codes_list), Func.lit(1)).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "# cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "test_cohort_df = cohort_df.filter(Func.col(\"CURRENT_OPIOID_IND\") == 1)\n",
    "\n",
    "# Show the filtered records\n",
    "test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code creates a patient-partitioned window to compute each encounter’s next start date \n",
    "    and the day difference between encounters, then assigns 90-day and 30-day readmission indicators, \n",
    "    conditionally replaces the next encounter start date with \"N/A\" when appropriate, renames that column to FIRST_READMISSION_DATE, \n",
    "    and finally displays filtered results.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window partitioned by pat_Id and ordered by enc_STOP (converted to timestamp)\n",
    "patient_window = Window.partitionBy(\"pat_Id\").orderBy(Func.col(\"enc_STOP\").cast(\"timestamp\"))\n",
    "\n",
    "# Get the next encounter's start date using lead().\n",
    "df_with_next = cohort_df.withColumn(\"next_enc_START\", Func.lead(\"enc_START\").over(patient_window))\n",
    "\n",
    "# Calculate the difference in days between the current encounter and the next encounter.\n",
    "df_with_diff = df_with_next.withColumn(\"diff_days\", Func.datediff(Func.col(\"next_enc_START\"), Func.col(\"enc_START\")))\n",
    "\n",
    "# Create the READMISSION_90_DAY_IND indicator: flag as 1 if the next encounter is within 90 days and not 0, else 0.\n",
    "df_with_indicator = df_with_diff.withColumn(\n",
    "    \"READMISSION_90_DAY_IND\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\").isNotNull()) &\n",
    "        (Func.col(\"diff_days\") != 0) &\n",
    "        (Func.col(\"diff_days\") <= 90), Func.lit(1)\n",
    "    ).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "#  create a 30-day readmission indicator.\n",
    "df_with_indicator = df_with_indicator.withColumn(\n",
    "    \"READMISSION_30_DAY_IND\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\").isNotNull()) &\n",
    "        (Func.col(\"diff_days\") != 0) &\n",
    "        (Func.col(\"diff_days\") <= 30), Func.lit(1)\n",
    "    ).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "# Show filtered results: those with diff_days between 1 and 89.\n",
    "df_with_indicator.filter(Func.col(\"diff_days\").between(1, 89)).show(5, truncate=False)\n",
    "df_with_indicator.filter(Func.col(\"READMISSION_30_DAY_IND\") == 1).show(5, truncate=False)\n",
    "\n",
    "# Update next_enc_START to \"N/A\" when diff_days is not positive or exceeds 90 (i.e. otherwise set to \"N/A\").\n",
    "df_with_indicator = df_with_indicator.withColumn(\n",
    "    \"next_enc_START\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\") <= 90) & (Func.col(\"diff_days\") != 0), Func.col(\"next_enc_START\")\n",
    "    ).otherwise(Func.lit(\"N/A\"))\n",
    ")\n",
    "\n",
    "# Rename next_enc_START to FIRST_READMISSION_DATE.\n",
    "df_with_indicator = df_with_indicator.withColumnRenamed('next_enc_START', 'FIRST_READMISSION_DATE')\n",
    "\n",
    "df_with_indicator.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_indicator.filter(F.col(\"diff_days\").between(1, 89)).show(5, truncate=False)\n",
    "df_with_indicator.filter(F.col(\"READMISSION_30_DAY_IND\") == 1).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d343467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Export the data to a CSV file\n",
    "# Select and rename columns to match the required format\n",
    "output_df = df_with_indicator.select(\n",
    "    F.col(\"pat_Id\").alias(\"PATIENT_ID\"),\n",
    "    F.col(\"enc_Id\").alias(\"ENCOUNTER_ID\"),\n",
    "    F.col(\"enc_START\").alias(\"HOSPITAL_ENCOUNTER_DATE\"),\n",
    "    F.col(\"AGE_AT_VISIT\"),\n",
    "    F.col(\"DEATH_AT_VISIT_IND\"),\n",
    "    F.col(\"COUNT_CURRENT_MEDS\"),\n",
    "    F.col(\"CURRENT_OPIOID_IND\"),\n",
    "    F.col(\"READMISSION_90_DAY_IND\"),\n",
    "    F.col(\"READMISSION_30_DAY_IND\"),\n",
    "    F.col(\"FIRST_READMISSION_DATE\")\n",
    ")\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "output_path = \"output/df_with_indicators.csv\"\n",
    "output_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"DataFrame written to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
