{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9550702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc784194",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ModularCSVLoader\").getOrCreate()\n",
    "\n",
    "# Base URL for all the CSV files\n",
    "base_url = \"datasets/\"\n",
    "\n",
    "# List of file names to process\n",
    "file_names = [\n",
    "\"allergies.csv\",\n",
    "\"encounters.csv\",\n",
    "\"medications.csv\",\n",
    "\"patients.csv\",\n",
    "\"procedures.csv\"\n",
    "]\n",
    "\n",
    "# Dictionaries to store the resulting DataFrames for further processing/joining.\n",
    "# pandas_dfs = {}\n",
    "spark_dfs = {}\n",
    "\n",
    "# Process each file and store the DataFrames into the dictionaries\n",
    "for file_name in file_names:\n",
    "    name_key = file_name.replace('.csv', '')\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    spark_df = spark.read.csv(file_url, header=True, inferSchema=True)\n",
    "    # pandas_dfs[name_key] = pd_df\n",
    "    spark_dfs[name_key] = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f47d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "enc_df = spark_dfs['encounters'].filter(\n",
    "    (spark_dfs['encounters'].REASONCODE == '55680006') &\n",
    "    (spark_dfs['encounters'].START > F.lit(\"1999-07-15 00:00:00\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "enc_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10012af",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = spark_dfs['patients'].filter(\n",
    "    (spark_dfs['patients'].BIRTHDATE.isNotNull())\n",
    ")\n",
    "\n",
    "patient_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "pat = patient_df.alias(\"pat\")\n",
    "enc = enc_df.alias(\"enc\")\n",
    "\n",
    "# Join on patient Id. Use proper aliases when referring to columns.\n",
    "joined_df = pat.join(enc, pat[\"Id\"] == enc[\"PATIENT\"], \"inner\")\n",
    "\n",
    "# Select the columns of interest from each DataFrame and rename encounter columns to avoid ambiguity.\n",
    "joined_df = joined_df.select(\n",
    "    pat[\"Id\"].alias(\"pat_Id\"), enc['id'].alias(\"enc_Id\"),\n",
    "    pat[\"BIRTHDATE\"], pat['DEATHDATE'],\n",
    "    enc[\"PATIENT\"].alias(\"enc_PATIENT_id\"),\n",
    "    enc[\"START\"].alias(\"enc_START\"),\n",
    "    enc[\"STOP\"].alias(\"enc_STOP\")\n",
    ")\n",
    "\n",
    "# Calculate the patient's age at the time of the encounter.\n",
    "# Assuming \"BIRTHDATE\" is in a format recognized by Spark for date conversion.\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"AGE_AT_VISIT\",\n",
    "    F.floor(F.datediff(F.col(\"enc_START\").cast(\"date\"), F.col(\"BIRTHDATE\").cast(\"date\")) / 365)\n",
    ")\n",
    "\n",
    "# Filter the records where age is between 18 and 35\n",
    "cohort_df = joined_df.filter((F.col(\"AGE_AT_VISIT\") >= 18) & (F.col(\"AGE_AT_VISIT\") <= 35))\n",
    "\n",
    "cohort_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eab1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cohort_df = cohort_df.withColumn(\n",
    "    \"DEATH_AT_VISIT_IND\",\n",
    "    Func.when(\n",
    "        # Check that DEATHDATE is not \"NA\" and falls between the START and STOP dates:\n",
    "        (Func.col(\"DEATHDATE\") != \"NA\") &\n",
    "        (Func.to_timestamp(Func.col(\"DEATHDATE\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\").between(Func.col('enc_START').cast(\"timestamp\"), Func.col('enc_STOP').cast(\"timestamp\"))),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# For debugging or previewing results:\n",
    "cohort_df.select(\"DEATHDATE\", \"DEATH_AT_VISIT_IND\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "test_cohort_df = cohort_df.filter(Func.col(\"DEATH_AT_VISIT_IND\") == 1)\n",
    "\n",
    "# Show the filtered records\n",
    "test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT_CURRENT_MEDS: Count of active medications at the start of the drug overdose encounter\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "med = spark_dfs['medications'].alias(\"m\")\n",
    "cohort = cohort_df.alias(\"c\")\n",
    "\n",
    "# Join the DataFrames using their aliases for clarity in the join condition\n",
    "med_df = med.join(cohort, F.col(\"c.pat_id\") == F.col(\"m.PATIENT\"), \"inner\") \\\n",
    "    .filter(\n",
    "        F.col(\"m.START\").cast(\"timestamp\") >= F.col(\"c.enc_START\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Now use the actual column names for renaming; after join, med.PATIENT will appear as \"PATIENT\"\n",
    "med_df = med_df.withColumnRenamed(\"PATIENT\", \"med_PATIENT\") \\\n",
    "    .withColumnRenamed(\"START\", \"med_START\") \\\n",
    "    .withColumnRenamed(\"STOP\", \"med_STOP\") \n",
    "\n",
    "grouped_med = med_df.groupBy(\n",
    "    \"CODE\", \"ENCOUNTER\", \"med_PATIENT\"\n",
    ").agg(F.count(\"*\").alias(\"med_cnt\"))\n",
    "\n",
    "grouped_med = grouped_med.groupBy(\"med_PATIENT\", \"ENCOUNTER\").agg(\n",
    "    F.sum(\"med_cnt\").alias(\"COUNT_CURRENT_MEDS\")\n",
    ")\n",
    "\n",
    "med_df = med_df.join(grouped_med,\n",
    "    (med_df[\"med_PATIENT\"] == grouped_med[\"med_PATIENT\"]) &\n",
    "    (med_df[\"ENCOUNTER\"] == grouped_med[\"ENCOUNTER\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    med_df[\"*\"],\n",
    "    grouped_med[\"COUNT_CURRENT_MEDS\"]\n",
    ")\n",
    "\n",
    "med_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuale and add CURRENT_OPIOID_IND\t\n",
    "# if the patient had at least one active medication at the start of the overdose encounter that is on the Opioids List (provided below)\n",
    "\n",
    "# GET CODES FOR  Opioids List:\n",
    "# Hydromorphone 325Mg\n",
    "# Fentanyl â€“ 100 MCG\n",
    "# Oxycodone-acetaminophen 100 Ml\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "patterns = [\n",
    "    \"(?i)^Hydromorphone 325\", \n",
    "    \"(?i)^Fentanyl\", \n",
    "    \"(?i)^Oxycodone-acetaminophen 100\"\n",
    "]\n",
    "\n",
    "# Build a filter condition by OR-ing each pattern on the DESCRIPTION column\n",
    "filter_condition = None\n",
    "for pattern in patterns:\n",
    "    cond = F.col(\"DESCRIPTION\").rlike(pattern)\n",
    "    filter_condition = cond if filter_condition is None else filter_condition | cond\n",
    "\n",
    "# Filter med_df using the combined condition and return only the distinct CODE column.\n",
    "# Then extract the CODE values as a Python list.\n",
    "opioid_codes_list = [row[\"CODE\"] for row in med_df.filter(filter_condition).select(\"CODE\").distinct().collect()]\n",
    "\n",
    "# Print the resulting list of opioid codes\n",
    "print(opioid_codes_list)\n",
    "\n",
    "# Add the CURRENT_OPIOID_IND column: 1 if med_df.CODE is in restricted_codes_list, else 0.\n",
    "cohort_df = med_df.withColumn(\n",
    "    \"CURRENT_OPIOID_IND\",\n",
    "    F.when(F.col(\"CODE\").isin(*opioid_codes_list), F.lit(1)).otherwise(F.lit(0))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "test_cohort_df = cohort_df.filter(Func.col(\"CURRENT_OPIOID_IND\") == 1)\n",
    "\n",
    "# Show the filtered records\n",
    "test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and add READMISSION_90_DAY_IND\t\n",
    "# Indicator if the visit resulted in a subsequent readmission within 90 days\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
