{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adba6574",
   "metadata": {},
   "source": [
    "\n",
    "**Summary of the PySpark Notebook:**\n",
    "\n",
    "The notebook implements a complete workflow to build a study cohort for patients seen for drug overdose and then calculate several key indicators related to those encounters. It begins by ingesting provided datasets (i.e patients, encounters, medications, allergies, and procedures) and uses a provided data dictionary to understand field definitions. The cohort is assembled by applying inclusion criteria: only encounters identified as drug overdose events that occurred after July 15, 1999, are included, and only patients aged between 18 and 35 at the time of the encounter are retained.\n",
    "\n",
    "Following cohort assembly, the notebook creates additional derived fields using PySpark’s DataFrame API and window functions. For example, it calculates:\n",
    "\n",
    "- **DEATH_AT_VISIT_IND:** A flag indicating whether the patient died during the overdose encounter versus at another time.\n",
    "- **COUNT_CURRENT_MEDS:** The count of active medications at the start of the overdose encounter.\n",
    "- **CURRENT_OPIOID_IND:** A binary indicator showing if at least one active medication is on a specified Opioids List.\n",
    "- **READMISSION_90_DAY_IND and READMISSION_30_DAY_IND:** Indicators calculated using lead functions and date difference calculations that flag whether the encounter was followed by a subsequent drug overdose readmission within 90 days or 30 days, respectively.\n",
    "- **FIRST_READMISSION_DATE:** This field is derived using window functions to extract the date of the first readmission within the 90-day window for each index encounter, with a value set to \"N/A\" if no such readmission exists.\n",
    "\n",
    "Finally, the notebook includes steps to filter, inspect, and summarize these computed metrics. The complete workflow is designed with modularity and reusability in mind, culminating in the export of the final cohort data to a CSV file for downstream analysis or reporting. This approach demonstrates reproducible ETL and advanced data manipulation methods using PySpark, consistent with the best practices outlined in the data exercise instructions.\n",
    "\n",
    "***More implementation details***\n",
    "  \n",
    "The notebook is a PySpark-based data processing workflow that loads clinical encounter data, performs data cleaning and joining, and then computes readmission indicators using window functions. In particular, it partitions the data by patient or encounter ID, orders records by encounter start or stop timestamps, and uses functions such as `lead()` and `datediff()` to calculate the time difference between successive encounters. Based on these differences, the notebook creates flags—for example, marking whether a readmission occurs within 30 or 90 days. It also demonstrates conditional column updates (for instance, replacing future encounter dates with \"N/A\" under certain conditions) and renames columns to standardize the output. Finally, the notebook includes steps to filter, display, and inspect intermediate results, effectively illustrating a complete ETL (Extract, Transform, Load) and analysis process for healthcare readmission data using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc784194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 18:55:19 WARN Utils: Your hostname, debian-shed resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface enp3s0)\n",
      "25/04/11 18:55:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 18:55:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: allergies.csv\n",
      "Processing file: encounters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: medications.csv\n",
      "Processing file: patients.csv\n",
      "Processing file: procedures.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"Import necessary libraries and modules.\n",
    "create spark session and load CSV files into DataFrames. in adictionary for further processing/joining.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ModularCSVLoader\").getOrCreate()\n",
    "\n",
    "# Base URL for all the CSV files\n",
    "base_url = \"datasets/\"\n",
    "\n",
    "# List of file names to process\n",
    "file_names = [\n",
    "\"allergies.csv\",\n",
    "\"encounters.csv\",\n",
    "\"medications.csv\",\n",
    "\"patients.csv\",\n",
    "\"procedures.csv\"\n",
    "]\n",
    "\n",
    "# Dictionaries to store the resulting DataFrames for further processing/joining.\n",
    "spark_dfs = {}\n",
    "\n",
    "# Process each file and store the DataFrames in a dictiona\n",
    "for file_name in file_names:\n",
    "    name_key = file_name.replace('.csv', '')\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    spark_df = spark.read.csv(file_url, header=True, inferSchema=True)\n",
    "    spark_dfs[name_key] = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f47d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Assemble the project cohort\n",
    "# Filter the 'encounters' DataFrame for specific conditions (e.g., REASONCODE and START date)\n",
    "\n",
    "enc_df = spark_dfs['encounters'].filter(\n",
    "    (spark_dfs['encounters'].REASONCODE == '55680006') &\n",
    "    (spark_dfs['encounters'].START > Func.lit(\"1999-07-15 00:00:00\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# enc_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10012af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a patients dataframe with only the patients that have a birthdate\n",
    "patient_df = spark_dfs['patients'].filter(\n",
    "    (spark_dfs['patients'].BIRTHDATE.isNotNull())\n",
    ")\n",
    "\n",
    "# patient_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bd6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 1: Assemble the project cohort\n",
    "The cohort is defined as patients aged 18-35 at the time of the encounter with a specific reason code.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "pat = patient_df.alias(\"pat\")\n",
    "enc = enc_df.alias(\"enc\")\n",
    "\n",
    "# Join on patient Id. Use proper aliases when referring to columns.\n",
    "joined_df = pat.join(enc, pat[\"Id\"] == enc[\"PATIENT\"], \"inner\")\n",
    "\n",
    "# Select and rename same name columns to avoid ambiguity.\n",
    "joined_df = joined_df.select(\n",
    "    pat[\"Id\"].alias(\"pat_Id\"), enc['id'].alias(\"enc_Id\"),\n",
    "    pat[\"BIRTHDATE\"], pat['DEATHDATE'],\n",
    "    enc[\"PATIENT\"].alias(\"enc_PATIENT_id\"),\n",
    "    enc[\"START\"].alias(\"enc_START\"),\n",
    "    enc[\"STOP\"].alias(\"enc_STOP\")\n",
    ")\n",
    "\n",
    "# Calculate the patient's age at the time of the encounter.\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"AGE_AT_VISIT\",\n",
    "    Func.floor(Func.datediff(Func.col(\"enc_START\").cast(\"date\"), Func.col(\"BIRTHDATE\").cast(\"date\")) / 365)\n",
    ")\n",
    "\n",
    "# Filter the records where age is between 18 and 35\n",
    "cohort_df = joined_df.filter((Func.col(\"AGE_AT_VISIT\") >= 18) & (Func.col(\"AGE_AT_VISIT\") <= 35))\n",
    "\n",
    "# cohort_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3eab1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "DEATH_AT_VISIT_IND: 1 if patient died during the drug overdose encounter, 0 if the patient died at a different time\n",
    "This cell adds a new column to the cohort DataFrame indicating whether the patient died during the encounter.\n",
    "\"\"\"\n",
    "\n",
    "cohort_df = cohort_df.withColumn(\n",
    "    \"DEATH_AT_VISIT_IND\",\n",
    "    Func.when(\n",
    "        # Check that DEATHDATE is not \"NA\" and falls between the START and STOP dates:\n",
    "        (Func.col(\"DEATHDATE\") != \"NA\") &\n",
    "        (Func.to_timestamp(Func.col(\"DEATHDATE\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\").between(Func.col('enc_START').cast(\"timestamp\"), Func.col('enc_STOP').cast(\"timestamp\"))),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# For debugging or previewing results:\n",
    "# cohort_df.select(\"DEATHDATE\", \"DEATH_AT_VISIT_IND\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af99f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes\n",
    "\n",
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "# test_cohort_df = cohort_df.filter(Func.col(\"DEATH_AT_VISIT_IND\") == 1)\n",
    "\n",
    "# # Show the filtered records\n",
    "# test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04efbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Create additional fields\n",
    "\n",
    "\"\"\"\n",
    "This cell counts the number of active medications at the start of the drug overdose encounter.\n",
    "It joins the medications DataFrame with the cohort DataFrame on patient ID and filters based on the encounter start date.\n",
    "The result is a DataFrame with the count of current medications for each patient at the time of the encounter.\n",
    "\"\"\"\n",
    "\n",
    "# Alias the DataFrames for clarity\n",
    "med = spark_dfs['medications'].alias(\"m\")\n",
    "cohort = cohort_df.alias(\"c\")\n",
    "\n",
    "# Join the DataFrames using their aliases for clarity in the join condition\n",
    "med_df = med.join(cohort, Func.col(\"c.pat_id\") == Func.col(\"m.PATIENT\"), \"inner\") \\\n",
    "    .filter(\n",
    "        Func.col(\"m.START\").cast(\"timestamp\") >= Func.col(\"c.enc_START\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Now use the actual column names for renaming; after join, med.PATIENT will appear as \"PATIENT\"\n",
    "med_df = med_df.withColumnRenamed(\"PATIENT\", \"med_PATIENT\") \\\n",
    "    .withColumnRenamed(\"START\", \"med_START\") \\\n",
    "    .withColumnRenamed(\"STOP\", \"med_STOP\") \n",
    "\n",
    "grouped_med = med_df.groupBy(\n",
    "    \"CODE\", \"ENCOUNTER\", \"med_PATIENT\"\n",
    ").agg(Func.count(\"*\").alias(\"med_cnt\"))\n",
    "\n",
    "grouped_med = grouped_med.groupBy(\"med_PATIENT\", \"ENCOUNTER\").agg(\n",
    "    Func.sum(\"med_cnt\").alias(\"COUNT_CURRENT_MEDS\")\n",
    ")\n",
    "\n",
    "med_df = med_df.join(grouped_med,\n",
    "    (med_df[\"med_PATIENT\"] == grouped_med[\"med_PATIENT\"]) &\n",
    "    (med_df[\"ENCOUNTER\"] == grouped_med[\"ENCOUNTER\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    med_df[\"*\"],\n",
    "    grouped_med[\"COUNT_CURRENT_MEDS\"]\n",
    ")\n",
    "\n",
    "# cohor and med df are now mergerd\n",
    "# med_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97cf0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316049, 429503]\n"
     ]
    }
   ],
   "source": [
    "# Calcuale and add CURRENT_OPIOID_IND\t\n",
    "# if the patient had at least one active medication at the start of the overdose encounter that is on the Opioids List (provided below)\n",
    "\n",
    "# GET CODES FOR  Opioids List:\n",
    "# Hydromorphone 325Mg\n",
    "# Fentanyl – 100 MCG\n",
    "# Oxycodone-acetaminophen 100 Ml\n",
    "\n",
    "# Define the list of opioid patterns to search for in the DESCRIPTION column\n",
    "patterns = [\n",
    "    \"(?i)^Hydromorphone 325\", \n",
    "    \"(?i)^Fentanyl\", \n",
    "    \"(?i)^Oxycodone-acetaminophen 100\"\n",
    "]\n",
    "\n",
    "# Build a filter condition by OR-ing each pattern on the DESCRIPTION column\n",
    "filter_condition = None\n",
    "for pattern in patterns:\n",
    "    cond = Func.col(\"DESCRIPTION\").rlike(pattern)\n",
    "    filter_condition = cond if filter_condition is None else filter_condition | cond\n",
    "\n",
    "# Filter med_df using the combined condition and return only the distinct CODE column.\n",
    "# Then extract the CODE values as a Python list.\n",
    "opioid_codes_list = [row[\"CODE\"] for row in med_df.filter(filter_condition).select(\"CODE\").distinct().collect()]\n",
    "\n",
    "# Print the resulting list of opioid codes\n",
    "print(opioid_codes_list)\n",
    "\n",
    "# Add the CURRENT_OPIOID_IND column: 1 if med_df.CODE is in restricted_codes_list, else 0.\n",
    "cohort_df = med_df.withColumn(\n",
    "    \"CURRENT_OPIOID_IND\",\n",
    "    Func.when(Func.col(\"CODE\").isin(*opioid_codes_list), Func.lit(1)).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "# cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd8f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for debuigging and testing purposes\n",
    "# Filter records where DEATH_AT_VISIT_IND = 1\n",
    "# test_cohort_df = cohort_df.filter(Func.col(\"CURRENT_OPIOID_IND\") == 1)\n",
    "\n",
    "# # Show the filtered records\n",
    "# test_cohort_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128ec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a patient-partitioned window to compute each encounter's next start date \n",
    "    and the day difference between encounters, then assigns 90-day and 30-day readmission indicators, \n",
    "    conditionally replaces the next encounter start date with \"N/A\" when appropriate, renames that column to FIRST_READMISSION_DATE, \n",
    "    and finally displays filtered results.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window partitioned by pat_Id and ordered by enc_STOP (converted to timestamp)\n",
    "patient_window = Window.partitionBy(\"pat_Id\").orderBy(Func.col(\"enc_STOP\").cast(\"timestamp\"))\n",
    "\n",
    "# Get the next encounter's start date using lead().\n",
    "df_with_next = cohort_df.withColumn(\"next_enc_START\", Func.lead(\"enc_START\").over(patient_window))\n",
    "\n",
    "# Calculate the difference in days between the current encounter and the next encounter.\n",
    "df_with_diff = df_with_next.withColumn(\"diff_days\", Func.datediff(Func.col(\"next_enc_START\"), Func.col(\"enc_START\")))\n",
    "\n",
    "# Create the READMISSION_90_DAY_IND indicator: flag as 1 if the next encounter is within 90 days and not 0, else 0.\n",
    "df_with_indicator = df_with_diff.withColumn(\n",
    "    \"READMISSION_90_DAY_IND\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\").isNotNull()) &\n",
    "        (Func.col(\"diff_days\") != 0) &\n",
    "        (Func.col(\"diff_days\") <= 90), Func.lit(1)\n",
    "    ).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "#  create a 30-day readmission indicator.\n",
    "df_with_indicator = df_with_indicator.withColumn(\n",
    "    \"READMISSION_30_DAY_IND\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\").isNotNull()) &\n",
    "        (Func.col(\"diff_days\") != 0) &\n",
    "        (Func.col(\"diff_days\") <= 30), Func.lit(1)\n",
    "    ).otherwise(Func.lit(0))\n",
    ")\n",
    "\n",
    "# # Debug filtered results: those with diff_days between 1 and 89.\n",
    "# df_with_indicator.filter(Func.col(\"diff_days\").between(1, 89)).show(5, truncate=False)\n",
    "# df_with_indicator.filter(Func.col(\"READMISSION_30_DAY_IND\") == 1).show(5, truncate=False)\n",
    "\n",
    "# Update next_enc_START to \"N/A\" when diff_days is not positive or exceeds 90 (i.e. otherwise set to \"N/A\").\n",
    "df_with_indicator = df_with_indicator.withColumn(\n",
    "    \"next_enc_START\",\n",
    "    Func.when(\n",
    "        (Func.col(\"diff_days\") <= 90) & (Func.col(\"diff_days\") != 0), Func.col(\"next_enc_START\")\n",
    "    ).otherwise(Func.lit(\"N/A\"))\n",
    ")\n",
    "\n",
    "# Rename next_enc_START to FIRST_READMISSION_DATE.\n",
    "df_with_indicator = df_with_indicator.withColumnRenamed('next_enc_START', 'FIRST_READMISSION_DATE')\n",
    "\n",
    "# df_with_indicator.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d5a1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for testing and debugging purposes\n",
    "# df_with_indicator.filter(F.col(\"diff_days\").between(1, 89)).show(5, truncate=False)\n",
    "# df_with_indicator.filter(F.col(\"READMISSION_30_DAY_IND\") == 1).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d343467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame written to output/df_with_indicators.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Export the data to a CSV file\n",
    "# Select and rename columns to match the required format\n",
    "output_df = df_with_indicator.select(\n",
    "    Func.col(\"pat_Id\").alias(\"PATIENT_ID\"),\n",
    "    Func.col(\"enc_Id\").alias(\"ENCOUNTER_ID\"),\n",
    "    Func.col(\"enc_START\").alias(\"HOSPITAL_ENCOUNTER_DATE\"),\n",
    "    Func.col(\"AGE_AT_VISIT\"),\n",
    "    Func.col(\"DEATH_AT_VISIT_IND\"),\n",
    "    Func.col(\"COUNT_CURRENT_MEDS\"),\n",
    "    Func.col(\"CURRENT_OPIOID_IND\"),\n",
    "    Func.col(\"READMISSION_90_DAY_IND\"),\n",
    "    Func.col(\"READMISSION_30_DAY_IND\"),\n",
    "    Func.col(\"FIRST_READMISSION_DATE\")\n",
    ")\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "output_path = \"output/df_with_indicators.csv\"\n",
    "output_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"DataFrame written to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
